{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Seden Canpolat 20070001044"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import jpype\n",
    "from jpype import JClass, getDefaultJVMPath, startJVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of Zemberek library for Turkish text processing\n",
    "\n",
    "zemberek_path = r'C:\\Users\\Seden\\Downloads\\zemberek-full.jar'\n",
    "\n",
    "startJVM(getDefaultJVMPath(), '-ea', f'-Djava.class.path={zemberek_path}')\n",
    "TurkishMorphology = JClass('zemberek.morphology.TurkishMorphology')\n",
    "TurkishTokenizer = JClass('zemberek.tokenization.TurkishTokenizer')\n",
    "morphology = TurkishMorphology.createWithDefaults()\n",
    "tokenizer = TurkishTokenizer.DEFAULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r'C:\\Users\\Seden\\Desktop\\SE4475_Assignment\\Assignment-data\\raw_texts'\n",
    "\n",
    "class_labels = {\"1\": \"Positive\", \"2\": \"Negative\", \"3\": \"Neutral\"}\n",
    "\n",
    "# Preprocessing tweets by removing punctuation and converting to lowercase, tokenizing, and stemming\n",
    "data = []\n",
    "\n",
    "for folder_name, label in class_labels.items():\n",
    "    class_dir = os.path.join(data_dir, folder_name)\n",
    "    for file_name in os.listdir(class_dir):\n",
    "        file_path = os.path.join(class_dir, file_name)\n",
    "        with open(file_path, \"r\") as file:\n",
    "            content = file.read().strip()\n",
    "        \n",
    "        if content:\n",
    "            text = re.sub(r'[^\\w\\s]', '', content).lower()\n",
    "            \n",
    "            tokens = list(tokenizer.tokenizeToStrings(text))\n",
    "            stemmed_tokens = []\n",
    "\n",
    "            for token in tokens:\n",
    "                analysis = morphology.analyzeAndDisambiguate(token).bestAnalysis()\n",
    "                if analysis and not analysis[0].isUnknown():\n",
    "                    stemmed_token = analysis[0].getLemmas()[0]\n",
    "                else:\n",
    "                    stemmed_token = token\n",
    "                stemmed_tokens.append(str(stemmed_token))\n",
    "            \n",
    "            data.append((stemmed_tokens, label))\n",
    "\n",
    "# Creating the DataFrame for processed tokens and saving them to CSV\n",
    "df = pd.DataFrame(data, columns=[\"processed_tokens\", \"class_label\"])\n",
    "\n",
    "output_path_tokenized = r\"C:\\Users\\Seden\\Desktop\\SE4475_Assignment\\all_tweets_tokenized.csv\"\n",
    "df.to_csv(output_path_tokenized, index=False, encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the data with encoding labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['numeric_class'] = label_encoder.fit_transform(df['class_label'])\n",
    "\n",
    "# Computing the TF-IDF matrix\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    lowercase=False,       # Already lowercased \n",
    "    tokenizer=lambda x:x,  # Using tokens directly \n",
    "    preprocessor=None,\n",
    "    token_pattern=None\n",
    ")\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['processed_tokens'])\n",
    "\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "# Adding class labels and saving TF-IDF results\n",
    "tfidf_df['Class'] = df['numeric_class']\n",
    "\n",
    "output_path_tfidf = r\"C:\\Users\\Seden\\Desktop\\SE4475_Assignment\\tf_idf_values.csv\"\n",
    "tfidf_df.to_csv(output_path_tfidf, index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of Stratified 10-Fold Cross-Validation \n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "X = tfidf_df.iloc[:, :-1].values  \n",
    "y = tfidf_df['Class'].values  \n",
    "\n",
    "k_values = range(1, 30)  \n",
    "\n",
    "num_classes = len(np.unique(y)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating k-NN performance with different k values\n",
    "for k in k_values:\n",
    "    fold_tp = np.zeros(num_classes)  # True Positives \n",
    "    fold_fp = np.zeros(num_classes)  # False Positives \n",
    "    fold_fn = np.zeros(num_classes)  # False Negatives \n",
    "\n",
    "    # Cross-validation\n",
    "    for train_idx, test_idx in skf.split(X, y):\n",
    "        train_X, test_X = X[train_idx], X[test_idx]\n",
    "        train_y, test_y = y[train_idx], y[test_idx]\n",
    "\n",
    "        # Computing cosine similarities for k-NN classification\n",
    "        dot_products = np.dot(test_X, train_X.T)  \n",
    "        test_norms = np.linalg.norm(test_X, axis=1, keepdims=True) \n",
    "        train_norms = np.linalg.norm(train_X, axis=1, keepdims=True)\n",
    "        train_norms = train_norms.T\n",
    "        cosine_similarities = dot_products / (test_norms * train_norms + 1e-8) \n",
    "\n",
    "        # Predicting labels using k-NN with majority voting\n",
    "        predictions = []\n",
    "        for i, similarity_vector in enumerate(cosine_similarities):\n",
    "            sorted_indices = np.argsort(similarity_vector)[::-1][:k]  \n",
    "            nearest_labels = train_y[sorted_indices]\n",
    "            predicted_class = np.bincount(nearest_labels).argmax()\n",
    "            predictions.append(predicted_class)\n",
    "\n",
    "        # Calculating performance metrics for each class\n",
    "        for clas in range(num_classes):\n",
    "            fold_tp[clas] += sum((test_y == clas) & (np.array(predictions) == clas))  # True Positives\n",
    "            fold_fp[clas] += sum((test_y != clas) & (np.array(predictions) == clas))  # False Positives\n",
    "            fold_fn[clas] += sum((test_y == clas) & (np.array(predictions) != clas))  # False Negatives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_macro_f1 = -np.inf\n",
    "\n",
    "# Calculating precision, recall, and F1-score for each class\n",
    "precision = fold_tp / (fold_tp + fold_fp + 1e-8)\n",
    "recall = fold_tp / (fold_tp + fold_fn + 1e-8)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "\n",
    "# Calculating macro\n",
    "macro_precision = np.mean(precision)\n",
    "macro_recall = np.mean(recall)\n",
    "macro_f1 = np.mean(f1_score)\n",
    "\n",
    "# Calculating micro\n",
    "total_tp = np.sum(fold_tp)\n",
    "total_fp = np.sum(fold_fp)\n",
    "total_fn = np.sum(fold_fn)\n",
    "\n",
    "micro_precision = total_tp / (total_tp + total_fp + 1e-8)\n",
    "micro_recall = total_tp / (total_tp + total_fn + 1e-8)\n",
    "micro_f1 = 2 * (micro_precision * micro_recall) / (micro_precision + micro_recall + 1e-8)\n",
    "\n",
    "# Tracking and storing the best k based on macro F1-score \n",
    "if macro_f1 > best_macro_f1:\n",
    "    best_k = k\n",
    "    best_macro_f1 = macro_f1\n",
    "    best_report = {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1_score,\n",
    "        \"macro_precision\": macro_precision,\n",
    "        \"macro_recall\": macro_recall,\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"micro_precision\": micro_precision,\n",
    "        \"micro_recall\": micro_recall,\n",
    "        \"micro_f1\": micro_f1,\n",
    "        \"tp\": fold_tp,\n",
    "        \"fp\": fold_fp,\n",
    "        \"fn\": fold_fn,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best results of k-NN obtained by: k = 29, similarity metric: Cosine Similarity\n",
      "\n",
      "Metric                    Class 1    Class 2    Class 3    MACRO Average   MICRO Average  \n",
      "Precision                 0.5780     0.5007     0.5030     0.5272          0.5435         \n",
      "Recall                    0.7366     0.3588     0.4484     0.5146          0.5435         \n",
      "F1_score                  0.6478     0.4180     0.4741     0.5133          0.5435         \n",
      "True Positives            948        343        339        -               -              \n",
      "False Positives           692        342        335        -               -              \n",
      "False Negatives           339        613        417        -               -              \n"
     ]
    }
   ],
   "source": [
    "csv_output_path = r\"C:\\Users\\Seden\\Desktop\\SE4475_Assignment\\knn_results_report.csv\"\n",
    "\n",
    "print(f\"\\nBest results of k-NN obtained by: k = {best_k}, similarity metric: Cosine Similarity\\n\")\n",
    "\n",
    "header = [\"Metric\", \"Class 1\", \"Class 2\", \"Class 3\", \"MACRO Average\", \"MICRO Average\"]\n",
    "print(\"{:<25} {:<10} {:<10} {:<10} {:<15} {:<15}\".format(*header))\n",
    "\n",
    "# Opening the CSV file for writing\n",
    "with open(csv_output_path, mode='w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"Best results of k-NN obtained by:\", f\"k = {best_k}\", \"Similarity metric: Cosine Similarity\"])\n",
    "    writer.writerow(header)\n",
    "\n",
    "    metrics = [\"precision\", \"recall\", \"f1_score\"]\n",
    "    \n",
    "    def get_average_key(metric, average_type):\n",
    "        key = f\"{average_type}_{metric}\"\n",
    "        return best_report.get(key, None)\n",
    "    \n",
    "    # Printing and saving rows for Precision, Recall, and F1-Score\n",
    "    for metric in metrics:\n",
    "        metric_key = metric\n",
    "        macro_avg = get_average_key(metric_key, \"macro\")\n",
    "        micro_avg = get_average_key(metric_key, \"micro\")\n",
    "\n",
    "        # Calculating macro average and micro average\n",
    "        if macro_avg is None and metric_key in best_report:\n",
    "            macro_avg = np.mean([best_report[metric_key][0], best_report[metric_key][1], best_report[metric_key][2]])\n",
    "\n",
    "        if micro_avg is None and metric_key in best_report:\n",
    "            total_tp = sum(best_report['tp'])\n",
    "            total_fn = sum(best_report['fn'])\n",
    "            micro_avg = total_tp / (total_tp + total_fn)\n",
    "\n",
    "        # Writing results to console and CSV\n",
    "        row = [\n",
    "            metric.capitalize(),\n",
    "            f\"{best_report[metric_key][0]:.4f}\",  \n",
    "            f\"{best_report[metric_key][1]:.4f}\",  \n",
    "            f\"{best_report[metric_key][2]:.4f}\",  \n",
    "            f\"{macro_avg:.4f}\" if macro_avg is not None else \"N/A\",  \n",
    "            f\"{micro_avg:.4f}\" if micro_avg is not None else \"N/A\",  \n",
    "        ]\n",
    "        print(\"{:<25} {:<10} {:<10} {:<10} {:<15} {:<15}\".format(*row))\n",
    "        writer.writerow(row)\n",
    "      \n",
    "\n",
    "    # Writing TP, FP, FN counts\n",
    "    counts = [\"True Positives\", \"False Positives\", \"False Negatives\"]\n",
    "    count_keys = [\"tp\", \"fp\", \"fn\"]\n",
    "    for i, count in enumerate(counts):\n",
    "        row = [\n",
    "            count,\n",
    "            int(best_report[count_keys[i]][0]),  \n",
    "            int(best_report[count_keys[i]][1]),  \n",
    "            int(best_report[count_keys[i]][2]),  \n",
    "            \"-\", \n",
    "            \"-\",  \n",
    "        ]\n",
    "        print(\"{:<25} {:<10} {:<10} {:<10} {:<15} {:<15}\".format(*row))\n",
    "        writer.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
